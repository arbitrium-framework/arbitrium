# Default model definitions
#
# This file contains ALL available models that can be used in tournaments.
# All models are defined here - you select which ones to use in your config.
#
# In your config file, just reference the models you want:
#   models:
#     gpt: {}          # Use defaults
#     claude: {}       # Use defaults
#     llama3:
#       temperature: 0.5  # Override specific setting
#
# Models are organized by size/tier for easy navigation.

models:
  # ============================================================================
  # Small models (2-4B parameters, 2-4GB RAM)
  # ============================================================================

  gemma-2b: # 2B parameters, ~2GB RAM
    provider: ollama
    model_name: ollama/gemma:2b
    display_name: "Gemma 2B"
    temperature: 0.7
    max_tokens: 4096  # Override auto-detected 2048 (25% of 8192)

  phi: # 2.7B parameters, ~3GB RAM
    provider: ollama
    model_name: ollama/phi
    display_name: "Phi-2 2.7B"
    temperature: 0.7
    max_tokens: 1024  # Override auto-detected 512 (25% of 2048)

  orca-mini: # 3B parameters, ~3GB RAM
    provider: ollama
    model_name: ollama/orca-mini:3b
    display_name: "Orca Mini 3B"
    temperature: 0.7
    max_tokens: 1024  # Override auto-detected 512 (25% of 2048)

  # ============================================================================
  # Medium-small models (3.8-4B parameters, 4GB RAM)
  # ============================================================================

  phi3: # 3.8B parameters, ~4GB RAM, context 4096
    provider: ollama
    model_name: ollama/phi3
    display_name: "Phi-3 Mini 3.8B"
    temperature: 0.7

  phi4-mini: # 3.8B parameters, ~4GB RAM
    provider: ollama
    model_name: ollama/phi4-mini
    display_name: "Phi-4 3.8b"
    temperature: 0.7

  gemma3-4b: # 4B parameters, ~4GB RAM
    provider: ollama
    model_name: ollama/gemma3:4b
    display_name: "Gemma 3 4B"
    temperature: 0.7

  qwen3-4b: # 4B parameters, ~4GB RAM, context 8192
    provider: ollama
    model_name: ollama/qwen3:4b
    display_name: "Qwen 3 4B"
    temperature: 0.7

  # ============================================================================
  # Standard-small models (7-12B parameters, 8-12GB RAM)
  # ============================================================================

  mistral-1: # 7B parameters, ~8GB RAM
    provider: ollama
    model_name: ollama/mistral:7b-instruct-v0.2-q4_K_M
    display_name: "Mistral 7B"
    temperature: 0.7

  llama3: # 8B parameters, ~8GB RAM
    provider: ollama
    model_name: ollama/llama3:8b-instruct-q4_K_M
    display_name: "Llama 3 8B"
    temperature: 0.7

  qwen3-8b: # 8B parameters, ~8GB RAM
    provider: ollama
    model_name: ollama/qwen3:8b
    display_name: "Qwen 3 8B"
    temperature: 0.7

  gemma3-12b: # 12B parameters, ~12GB RAM
    provider: ollama
    model_name: ollama/gemma3:12b
    display_name: "Gemma 3 12B"
    temperature: 0.7

  # ============================================================================
  # Medium models (14-30B parameters, 16-32GB RAM)
  # ============================================================================

  phi4: # 14B parameters, ~16GB RAM
    provider: ollama
    model_name: ollama/phi4
    display_name: "Phi-4 14b"
    temperature: 0.7

  gpt-oss-20b: # 20B parameters, ~24GB RAM
    provider: ollama
    model_name: ollama/gpt-oss:20b-cloud
    display_name: "GPT-OSS 20B"
    temperature: 0.7

  gemma3-27b: # 27B parameters, ~32GB RAM
    provider: ollama
    model_name: ollama/gemma3:27b
    display_name: "Gemma 3 27B"
    temperature: 0.7

  qwen3-30b: # 30B parameters, ~32GB RAM
    provider: ollama
    model_name: ollama/qwen3:30b
    display_name: "Qwen 3 30B"
    temperature: 0.7

  # ============================================================================
  # Huge/Semi-clever models (120B-671B parameters, >128GB RAM)
  # ============================================================================

  gpt-oss-120b: # 120B parameters, ~128GB RAM
    provider: ollama
    model_name: ollama/gpt-oss:120b-cloud
    display_name: "GPT-OSS 120B"
    temperature: 0.7

  deepseek-v3: # 671B parameters, >512GB RAM
    provider: ollama
    model_name: ollama/deepseek-v3.1:671b-cloud
    display_name: "DeepSeek V3 671B"
    temperature: 0.7
    context_window: 128000
    max_tokens: 8192

  grok: # Parameters not public but constantly scored lower than top tier
    provider: xai
    model_name: xai/grok-4-latest
    display_name: "Grok 4"
    temperature: 0.7

  # ============================================================================
  # Top tier models (Parameters not public - SOTA commercial models)
  # ============================================================================

  gpt: # Parameters not public
    provider: openai
    model_name: gpt-5
    display_name: "GPT-5"
    temperature: 1.0  # GPT-5 only supports temperature=1.0
    reasoning_effort: "high"

  claude: # Parameters not public
    provider: anthropic
    model_name: claude-sonnet-4-5-20250929
    display_name: "Claude 4.5 Sonnet"
    temperature: 1.0
    reasoning_effort: "high"

  gemini: # Parameters not public
    provider: vertex_ai
    model_name: vertex_ai/gemini-2.5-pro
    display_name: "Gemini 2.5 Pro"
    temperature: 0.7
    reasoning_effort: "high"

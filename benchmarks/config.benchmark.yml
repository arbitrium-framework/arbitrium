question: >-
  Identify Python code quality tools with proven, measurable impact on codebase health,
  grounded in empirical evidence and sound engineering principles.
  Focus on tools that go beyond basic formatting and linting - prioritize those with
  demonstrated effectiveness in detecting structural issues, architectural problems,
  security vulnerabilities, DRY violations, logic errors, and maintainability risks.
  Consider both cutting-edge research-backed technologies and battle-tested fundamental
  tools with verified track records.
  Exclude tools based on hype or trends without substantiated benefits. Include only
  those with clear, falsifiable claims about their capabilities and measurable
  improvements to code quality metrics.
  For reference, here is a baseline list of commonly mentioned tools - critically
  evaluate which truly deliver value versus which are merely popular:
  Bandit, Black, Coverage.py, cProfile, Hypothesis, line_profiler, MutPy, Mypy,
  Pydantic, Pyinstrument, Pylint, Pyright, pytest, Pyupgrade, Radon, Rosebud, Ruff,
  Safety, Semgrep, SonarQube / SonarCloud (SonarPython), Sourcery, Unused

models:
  # Tiny models DONT USE THEM THEY WON'T WORK (< 2B parameters, ~1-2GB RAM)

  # Small models

  # gemma-2b: # 2B parameters, ~2GB RAM
  #   provider: ollama
  #   model_name: ollama/gemma:2b
  #   display_name: "Gemma 2B"
  #   temperature: 1.0

  # phi: # 2.7B parameters, ~3GB RAM
  #   provider: ollama
  #   model_name: ollama/phi
  #   display_name: "Phi-2 2.7B"
  #   temperature: 1.0

  # orca-mini: # 3B parameters, ~3GB RAM
  #   provider: ollama
  #   model_name: ollama/orca-mini:3b
  #   display_name: "Orca Mini 3B"
  #   temperature: 1.0

  # Medium-small models

  # phi3: # 3.8B parameters, ~4GB RAM
  #   provider: ollama
  #   model_name: ollama/phi3
  #   display_name: "Phi-3 Mini 3.8B"
  #   temperature: 0.7

  # phi4-mini: # 3.8B parameters, ~4GB RAM
  #   provider: ollama
  #   model_name: ollama/phi4-mini
  #   display_name: "Phi-4 3.8b"
  #   temperature: 0.7

  # gemma3-4b: # 4B parameters, ~4GB RAM
  #   provider: ollama
  #   model_name: ollama/gemma3:4b
  #   display_name: "Gemma 3 4B"
  #   temperature: 0.7

  # qwen3-4b: # 4B parameters, ~4GB RAM
  #   provider: ollama
  #   model_name: ollama/qwen3:4b
  #   display_name: "Qwen 3 4B"
  #   temperature: 0.7

  # Standard-small models

  # mistral-1: # 7B parameters, ~8GB RAM
  #   provider: ollama
  #   model_name: ollama/mistral:7b-instruct-v0.2-q4_K_M
  #   display_name: "Mistral 7B"
  #   temperature: 0.7

  # llama3: # 8B parameters, ~8GB RAM
  #   provider: ollama
  #   model_name: ollama/llama3:8b-instruct-q4_K_M
  #   display_name: "Llama 3 8B"
  #   temperature: 0.7

  # qwen3-8b: # 8B parameters, ~8GB RAM
  #   provider: ollama
  #   model_name: ollama/qwen3:8b
  #   display_name: "Qwen 3 8B"
  #   temperature: 0.7

  # gemma3-12b: # 12B parameters, ~12GB RAM
  #   provider: ollama
  #   model_name: ollama/gemma3:12b
  #   display_name: "Gemma 3 12B"
  #   temperature: 0.7

  # Medium models

  # phi4: # 14B parameters, ~16GB RAM
  #   provider: ollama
  #   model_name: ollama/phi4
  #   display_name: "Phi-4 14b"
  #   temperature: 0.7

  # gpt-oss-20b: # 20B parameters, ~24GB RAM
  #   provider: ollama
  #   model_name: ollama/gpt-oss:20b-cloud
  #   display_name: "GPT-OSS 20B"
  #   temperature: 0.7

  # gemma3-27b: # 27B parameters, ~32GB RAM
  #   provider: ollama
  #   model_name: ollama/gemma3:27b
  #   display_name: "Gemma 3 27B"
  #   temperature: 0.7

  # qwen3-30b: # 30B parameters, ~32GB RAM
  #   provider: ollama
  #   model_name: ollama/qwen3:30b
  #   display_name: "Qwen 3 30B"
  #   temperature: 0.7

  # Huge/Semi clever models

  # gpt-oss-120b: # 120B parameters, ~128GB RAM
  #   provider: ollama
  #   model_name: ollama/gpt-oss:120b-cloud
  #   display_name: "GPT-OSS 120B"
  #   temperature: 0.7

  # deepseek-v3: # 671B parameters, >512GB RAM
  #   provider: ollama
  #   model_name: ollama/deepseek-v3.1:671b-cloud
  #   display_name: "DeepSeek V3 671B"
  #   temperature: 0.7
  #   context_window: 128000
  #   max_tokens: 8192

  # grok: # Parameters not public but constantly scored lower than top tier models
  #   provider: xai
  #   model_name: xai/grok-4-latest
  #   display_name: "Grok 4"
  #   temperature: 0.7

  # Top tier models

  gpt: # Parameters not public
    provider: openai
    model_name: gpt-5
    display_name: "GPT-5"
    temperature: 0.7
    reasoning_effort: "high"

  claude: # Parameters not public
    provider: anthropic
    model_name: claude-sonnet-4-5-20250929
    display_name: "Claude 4.5 Sonnet"
    temperature: 1.0
    reasoning_effort: "high"

  gemini: # Parameters not public
    provider: vertex_ai
    model_name: vertex_ai/gemini-2.5-pro
    display_name: "Gemini 2.5 Pro"
    temperature: 0.7
    reasoning_effort: "high"

retry:
  max_attempts: 2
  initial_delay: 5
  max_delay: 30

features:
  save_reports_to_disk: false
  deterministic_mode: true
  judge_model: null
  knowledge_bank_model: "leader"
  llm_compression: true
  compression_model: "ollama/gemma:2b"

knowledge_bank:
  enabled: true
  similarity_threshold: 0.75
  max_insights_to_inject: 50 # TODO: sort by importance

model_defaults:
  concurrency_limits:
    max_concurrent_requests: 60

prompts:
  initial: >-
    Analyze the problem from multiple perspectives using evidence-based reasoning.
    Be aware of your inherent biases and actively work to counteract them.
    Don't be constrained by current trends or prevailing narratives - trends change,
    human priorities shift, and emotional reactions should not limit objective analysis.
    Identify the core dilemma and outline several distinct, well-reasoned strategies
    grounded in scientific principles and common sense. Avoid unfalsifiable claims,
    pseudoscience, and speculative nonsense. Think critically and don't hesitate to
    challenge assumptions as the question can be self-restrictive. Be concise and specific.
  feedback: >-
    Provide feedback that will allow the one to improve.
    Identify the most insightful, evidence-based ideas that stand out.
    Be aware of confirmation bias and other cognitive biases when evaluating.
    Don't favor responses that merely align with popular sentiment - value independent
    thinking that challenges current trends when evidence supports it.
    Distinguish between verifiable insights and mere speculation.
    Note which elements rely on common consensus versus proven methodology.
    Be concise and specific.
  improvement: >-
    Improve the answer using feedback, grounding it in scientific evidence and
    practical reasoning. Recognize and mitigate your own biases in the analysis.
    Don't let emotional appeals or fashionable opinions constrain rigorous thinking.
    Make the key verifiable insights the central thesis.
    Rebuild the entire argument around this main point, removing all generic claims,
    unsubstantiated speculation, and secondary details. Be concise and specific.
  evaluate: >-
    You are an editor judging analytical depth and scientific rigor. How insightful
    and evidence-based is this answer? Does it rely on proven methodology and sound
    reasoning, or does it resort to speculation and unfalsifiable claims?
    Be aware that all analysis contains inherent biases - evaluate whether the answer
    acknowledges and addresses its own potential biases. Does it demonstrate intellectual
    independence by challenging prevailing trends when warranted, or does it merely
    echo popular sentiment? Be critical of both originality and factual grounding.
    Be concise and specific.

secrets:
  providers:
    openai:
      env_var: OPENAI_API_KEY
      op_path: "op://Personal/OpenAI/api-key"
    anthropic:
      env_var: ANTHROPIC_API_KEY
      op_path: "op://Personal/Anthropic/api-key"
    vertex_ai:
      env_var: VERTEX_AI_API_KEY
      op_path: "op://Personal/VertexAI/api-key"
    xai:
      env_var: XAI_API_KEY
      op_path: "op://Personal/XAI/api-key"

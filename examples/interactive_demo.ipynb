{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÜ Arbitrium Framework - Interactive Browser Demo\n",
    "\n",
    "## Tournament-Based AI Decision Synthesis - Live in Your Browser!\n",
    "\n",
    "Welcome! This interactive notebook lets you run **real** Arbitrium tournaments with **actual AI models** right here in your browser.\n",
    "\n",
    "**What you'll experience:**\n",
    "- ü§ñ Real AI models (GPT-4, Claude, Gemini) competing on your question\n",
    "- üîÑ Live tournament execution with real API calls\n",
    "- ‚öîÔ∏è Competitive elimination rounds\n",
    "- üß† Knowledge Bank preserving insights from eliminated models\n",
    "- ü•á Champion solution synthesizing the best ideas\n",
    "- üí∞ Real cost tracking and metrics\n",
    "\n",
    "**Requirements:**\n",
    "- Your own API keys (OpenAI, Anthropic, or Google)\n",
    "- ~$0.50-2.00 per tournament (depending on models used)\n",
    "- 5-10 minutes runtime\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Arbitrium Framework\n",
    "\n",
    "First, let's install the framework. In Pyodide/JupyterLite environments, use `micropip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation - works for Colab, JupyterLite, and local Jupyter\n",
    "\n",
    "# Try to import first (in case already installed)\n",
    "try:\n",
    "    import arbitrium\n",
    "    print(f\"‚úÖ Arbitrium already installed (v{arbitrium.__version__})\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing Arbitrium Framework...\\n\")\n",
    "    \n",
    "    # Try PyPI first (once published)\n",
    "    try:\n",
    "        # For Pyodide/JupyterLite\n",
    "        try:\n",
    "            import micropip\n",
    "            await micropip.install('arbitrium-framework')\n",
    "            print(\"‚úÖ Installed via micropip (Pyodide environment)\")\n",
    "        except:\n",
    "            # For regular Python/Colab\n",
    "            !{sys.executable} -m pip install -q arbitrium\n",
    "            print(\"‚úÖ Installed via pip\")\n",
    "        \n",
    "        import arbitrium\n",
    "        print(f\"\\nüéâ Arbitrium Framework v{arbitrium.__version__} ready!\")\n",
    "        \n",
    "    except Exception:\n",
    "        # Fallback: Install from GitHub (if not yet on PyPI)\n",
    "        print(\"‚ö†Ô∏è  Not found on PyPI, installing from GitHub...\")\n",
    "        !{sys.executable} -m pip install -q git+https://github.com/arbitrium-framework/arbitrium.git\n",
    "        import arbitrium\n",
    "        print(f\"‚úÖ Installed from GitHub (v{arbitrium.__version__})\\n\")\n",
    "\n",
    "# Import and verify\n",
    "import arbitrium\n",
    "print(\"üéâ Ready to run tournaments!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure Your API Keys\n",
    "\n",
    "**Security Note:** Your API keys stay in your browser session and are never sent anywhere except directly to the AI providers.\n",
    "\n",
    "Choose which models you want to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "print(\"üîê API Key Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nEnter API keys for the models you want to use.\")\n",
    "print(\"Press Enter to skip a provider.\\n\")\n",
    "\n",
    "# OpenAI (GPT-4)\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    openai_key = getpass.getpass(\"OpenAI API Key (for GPT-4): \")\n",
    "    if openai_key.strip():\n",
    "        os.environ['OPENAI_API_KEY'] = openai_key.strip()\n",
    "        print(\"  ‚úÖ OpenAI configured\")\n",
    "    else:\n",
    "        print(\"  ‚è≠Ô∏è  OpenAI skipped\")\n",
    "else:\n",
    "    print(\"  ‚úÖ OpenAI already configured\")\n",
    "\n",
    "# Anthropic (Claude)\n",
    "if not os.getenv('ANTHROPIC_API_KEY'):\n",
    "    anthropic_key = getpass.getpass(\"Anthropic API Key (for Claude): \")\n",
    "    if anthropic_key.strip():\n",
    "        os.environ['ANTHROPIC_API_KEY'] = anthropic_key.strip()\n",
    "        print(\"  ‚úÖ Anthropic configured\")\n",
    "    else:\n",
    "        print(\"  ‚è≠Ô∏è  Anthropic skipped\")\n",
    "else:\n",
    "    print(\"  ‚úÖ Anthropic already configured\")\n",
    "\n",
    "# Google (Gemini)\n",
    "if not os.getenv('GOOGLE_API_KEY'):\n",
    "    google_key = getpass.getpass(\"Google API Key (for Gemini): \")\n",
    "    if google_key.strip():\n",
    "        os.environ['GOOGLE_API_KEY'] = google_key.strip()\n",
    "        print(\"  ‚úÖ Google configured\")\n",
    "    else:\n",
    "        print(\"  ‚è≠Ô∏è  Google skipped\")\n",
    "else:\n",
    "    print(\"  ‚úÖ Google already configured\")\n",
    "\n",
    "# XAI (Grok) - optional\n",
    "if not os.getenv('XAI_API_KEY'):\n",
    "    xai_key = getpass.getpass(\"XAI API Key (for Grok, optional): \")\n",
    "    if xai_key.strip():\n",
    "        os.environ['XAI_API_KEY'] = xai_key.strip()\n",
    "        print(\"  ‚úÖ XAI configured\")\n",
    "    else:\n",
    "        print(\"  ‚è≠Ô∏è  XAI skipped\")\n",
    "else:\n",
    "    print(\"  ‚úÖ XAI already configured\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Configuration complete!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Tournament Configuration\n",
    "\n",
    "Let's build a configuration dynamically based on which API keys you provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Build model configuration based on available API keys\n",
    "models_config = {}\n",
    "\n",
    "if os.getenv('OPENAI_API_KEY'):\n",
    "    models_config['gpt4'] = {\n",
    "        'provider': 'openai',\n",
    "        'model_name': 'gpt-4-turbo',\n",
    "        'display_name': 'GPT-4 Turbo'\n",
    "    }\n",
    "\n",
    "if os.getenv('ANTHROPIC_API_KEY'):\n",
    "    models_config['claude'] = {\n",
    "        'provider': 'anthropic',\n",
    "        'model_name': 'claude-3-5-sonnet-20241022',\n",
    "        'display_name': 'Claude 3.5 Sonnet'\n",
    "    }\n",
    "\n",
    "if os.getenv('GOOGLE_API_KEY'):\n",
    "    models_config['gemini'] = {\n",
    "        'provider': 'gemini',\n",
    "        'model_name': 'gemini-1.5-pro',\n",
    "        'display_name': 'Gemini 1.5 Pro'\n",
    "    }\n",
    "\n",
    "if os.getenv('XAI_API_KEY'):\n",
    "    models_config['grok'] = {\n",
    "        'provider': 'xai',\n",
    "        'model_name': 'grok-2',\n",
    "        'display_name': 'Grok 2'\n",
    "    }\n",
    "\n",
    "# ‚ö†Ô∏è VALIDATION: Check for sufficient models\n",
    "if not models_config:\n",
    "    raise ValueError(\n",
    "        \"‚ùå No API keys configured! Please run Step 2 and provide at least one API key.\"\n",
    "    )\n",
    "\n",
    "if len(models_config) < 2:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: INSUFFICIENT MODELS FOR TOURNAMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\n‚ùå Found only {len(models_config)} active model(s):\")\n",
    "    for _key, model in models_config.items():\n",
    "        print(f\"   ‚Ä¢ {model['display_name']}\")\n",
    "    print(\"\\nüèÜ Arbitrium tournaments require at least 2 models to compete.\")\n",
    "    print(\"\\nüí° To fix this:\")\n",
    "    print(\"   1. ‚¨ÜÔ∏è  Scroll up to Step 2\")\n",
    "    print(\"   2. Re-run that cell and enter at least one more API key\")\n",
    "    print(\"   3. Then come back and re-run Steps 3-6\")\n",
    "    print(\"\\nüîë Need API keys? Get them here:\")\n",
    "    print(\"   ‚Ä¢ OpenAI:    https://platform.openai.com/api-keys\")\n",
    "    print(\"   ‚Ä¢ Anthropic: https://console.anthropic.com/settings/keys\")\n",
    "    print(\"   ‚Ä¢ Google:    https://aistudio.google.com/app/apikey\")\n",
    "    print(\"   ‚Ä¢ XAI:       https://console.x.ai/\")\n",
    "    print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "    raise ValueError(\n",
    "        f\"Tournament requires 2+ models, but only {len(models_config)} configured. \"\n",
    "        \"Please add more API keys in Step 2.\"\n",
    "    )\n",
    "\n",
    "# Create temporary directory for outputs\n",
    "temp_dir = tempfile.mkdtemp(prefix=\"arbitrium_demo_\")\n",
    "\n",
    "# Full configuration\n",
    "config = {\n",
    "    'models': models_config,\n",
    "    'outputs_dir': temp_dir,\n",
    "    'retry': {\n",
    "        'max_attempts': 3,\n",
    "        'initial_delay': 1,\n",
    "        'max_delay': 10,\n",
    "        'exponential_base': 2\n",
    "    },\n",
    "    'features': {\n",
    "        'save_reports_to_disk': True,\n",
    "        'deterministic_mode': False,\n",
    "        'judge_model': None,  # Use peer review\n",
    "        'knowledge_bank_model': 'leader',\n",
    "        'llm_compression': False\n",
    "    },\n",
    "    'knowledge_bank': {\n",
    "        'enabled': True,\n",
    "        'similarity_threshold': 0.75,\n",
    "        'max_insights_to_inject': 5\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üéØ Tournament Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìä Active Models ({len(models_config)}):\")\n",
    "for _key, model in models_config.items():\n",
    "    print(f\"  ‚Ä¢ {model['display_name']} ({model['model_name']})\")\n",
    "\n",
    "print(f\"\\nüß† Knowledge Bank: {'Enabled' if config['knowledge_bank']['enabled'] else 'Disabled'}\")\n",
    "print(f\"üìÅ Output Directory: {temp_dir}\")\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Initialize Arbitrium Framework\n",
    "\n",
    "Now let's initialize the framework and perform health checks on all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arbitrium import Arbitrium\n",
    "\n",
    "print(\"üèÅ Initializing Arbitrium Framework...\\n\")\n",
    "\n",
    "# Initialize from settings\n",
    "arbitrium = await Arbitrium.from_settings(\n",
    "    settings=config,\n",
    "    skip_secrets=True,  # We already set environment variables\n",
    "    skip_health_check=False  # DO perform health checks\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä Model Health Check Results\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Healthy Models: {arbitrium.healthy_model_count}\")\n",
    "if arbitrium.healthy_model_count > 0:\n",
    "    for model_key in arbitrium.healthy_models.keys():\n",
    "        model = arbitrium.healthy_models[model_key]\n",
    "        print(f\"   ‚Ä¢ {model.full_display_name}\")\n",
    "\n",
    "if arbitrium.failed_model_count > 0:\n",
    "    print(f\"\\n‚ùå Failed Models: {arbitrium.failed_model_count}\")\n",
    "    for model_key, error in arbitrium.failed_models.items():\n",
    "        print(f\"   ‚Ä¢ {model_key}: {error}\")\n",
    "    print(\"\\n‚ö†Ô∏è  Continuing with healthy models only.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "if arbitrium.healthy_model_count < 2:\n",
    "    raise ValueError(\n",
    "        \"‚ùå Need at least 2 healthy models to run a tournament! \"\n",
    "        \"Please check your API keys and try again.\"\n",
    "    )\n",
    "\n",
    "print(f\"üéâ Ready to run tournament with {arbitrium.healthy_model_count} models!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define Your Strategic Question\n",
    "\n",
    "Choose a question for the tournament. This should be a high-stakes, complex decision where multiple perspectives add value.\n",
    "\n",
    "**Examples:**\n",
    "- Strategic business decisions\n",
    "- Technical architecture choices\n",
    "- Market analysis and positioning\n",
    "- Product roadmap prioritization\n",
    "\n",
    "**Edit the question below or use the example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your question here\n",
    "question = \"\"\"\n",
    "Analyze the strategic decision: Should our SaaS startup focus on \n",
    "product-led growth (PLG) or sales-led growth (SLG) for our first year?\n",
    "\n",
    "Context:\n",
    "- Pre-revenue, team of 3 engineers\n",
    "- B2B product (developer tools for API testing)\n",
    "- $500k seed funding, 12-month runway\n",
    "- Competitive landscape: established players (Postman, Insomnia)\n",
    "- Target: 10,000 users or $500k ARR by end of year\n",
    "\n",
    "Consider:\n",
    "- Customer acquisition costs\n",
    "- Time to first revenue\n",
    "- Scalability with small team\n",
    "- Market expectations in this category\n",
    "- Risk vs. speed trade-offs\n",
    "\"\"\".strip()\n",
    "\n",
    "print(\"‚ùì Tournament Question\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{question}\\n\")\n",
    "print(\"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run the Tournament! üèÜ\n",
    "\n",
    "This will execute the full tournament:\n",
    "1. **Phase 1**: Each model generates independent initial responses\n",
    "2. **Phase 2**: Models improve based on peer feedback\n",
    "3. **Phase 3**: Cross-evaluation and elimination\n",
    "4. **Repeat**: Until one champion remains\n",
    "\n",
    "**Expected runtime:** 5-10 minutes (depending on number of models)\n",
    "\n",
    "**Expected cost:** $0.50-$2.00 (you'll see exact costs below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üèÜ ARBITRIUM TOURNAMENT - LIVE EXECUTION\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the tournament with REAL API calls\n",
    "result, metrics = await arbitrium.run_tournament(question)\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üèÅ TOURNAMENT COMPLETE\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "print(f\"‚è±Ô∏è  Duration: {duration:.1f} seconds ({duration/60:.1f} minutes)\")\n",
    "print(f\"üí∞ Total Cost: ${metrics['total_cost']:.4f}\")\n",
    "print(f\"ü•á Champion: {metrics['champion_model']}\")\n",
    "print(f\"üóëÔ∏è  Eliminated: {len(metrics['eliminated_models'])} models\")\n",
    "\n",
    "if metrics['eliminated_models']:\n",
    "    print(\"\\n   Elimination order:\")\n",
    "    for i, model in enumerate(metrics['eliminated_models'], 1):\n",
    "        print(f\"   {i}. {model}\")\n",
    "\n",
    "print(\"\\nüí∏ Cost Breakdown:\")\n",
    "for model, cost in sorted(metrics['cost_by_model'].items(), key=lambda x: x[1], reverse=True):\n",
    "    pct = (cost / metrics['total_cost'] * 100) if metrics['total_cost'] > 0 else 0\n",
    "    print(f\"   {model:20} ${cost:7.4f} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: View Champion Solution\n",
    "\n",
    "Here's the winning answer that synthesizes insights from all competing models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìú CHAMPION SOLUTION\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "display(Markdown(result))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Download Tournament Reports\n",
    "\n",
    "Arbitrium Framework generates detailed reports you can download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "print(\"üìÅ Tournament Reports\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# Find generated reports\n",
    "report_files = glob.glob(f\"{temp_dir}/arbitrium_*.json\") + glob.glob(f\"{temp_dir}/arbitrium_*.md\")\n",
    "\n",
    "if report_files:\n",
    "    print(f\"‚úÖ Generated {len(report_files)} report files:\\n\")\n",
    "    for filepath in sorted(report_files):\n",
    "        filename = Path(filepath).name\n",
    "        size_kb = Path(filepath).stat().st_size / 1024\n",
    "        print(f\"   üìÑ {filename} ({size_kb:.1f} KB)\")\n",
    "    \n",
    "    print(f\"\\nüìÇ Location: {temp_dir}\")\n",
    "    print(\"\\nüí° Tip: These files contain:\")\n",
    "    print(\"   ‚Ä¢ Complete tournament history (all responses, scores)\")\n",
    "    print(\"   ‚Ä¢ Provenance tracking (how ideas evolved)\")\n",
    "    print(\"   ‚Ä¢ Knowledge Bank insights\")\n",
    "    print(\"   ‚Ä¢ Cost breakdown by phase\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No report files found (may be disabled in config)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Compare with Single Model (Optional)\n",
    "\n",
    "Want to see how the tournament compares to just asking one model? Let's run the same question through a single model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose first healthy model for comparison\n",
    "comparison_model_key = next(iter(arbitrium.healthy_models.keys()))\n",
    "comparison_model = arbitrium.healthy_models[comparison_model_key]\n",
    "\n",
    "print(f\"ü§ñ Running single model comparison: {comparison_model.full_display_name}\\n\")\n",
    "\n",
    "single_response = await arbitrium.run_single_model(comparison_model_key, question)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"üìù SINGLE MODEL RESPONSE ({comparison_model.full_display_name})\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "display(Markdown(single_response.content))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚öñÔ∏è  COMPARISON\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "print(f\"Tournament Champion: {metrics['champion_model']}\")\n",
    "print(f\"Tournament Cost: ${metrics['total_cost']:.4f}\")\n",
    "print(f\"Tournament Time: {duration:.1f}s\")\n",
    "print(f\"\\nSingle Model: {comparison_model.full_display_name}\")\n",
    "print(f\"Single Model Cost: ${single_response.cost:.4f}\")\n",
    "print(f\"\\nüí∞ Cost Multiplier: {metrics['total_cost'] / single_response.cost:.1f}x\")\n",
    "print(\"\\nüí° Analysis: Is the tournament answer worth the extra cost?\")\n",
    "print(\"   Read both answers above and decide!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Try Your Own Question!\n",
    "\n",
    "Now that you've seen it work, modify the question in Step 5 and run Steps 6-9 again with your own strategic problem.\n",
    "\n",
    "**Tips for good tournament questions:**\n",
    "- Complex with multiple valid approaches\n",
    "- High-stakes ($5,000+ impact)\n",
    "- Benefits from diverse perspectives\n",
    "- Has context and constraints\n",
    "- No single \"obviously correct\" answer\n",
    "\n",
    "**Poor tournament questions:**\n",
    "- Simple factual queries\n",
    "- Questions with objective right answers\n",
    "- Very broad without constraints\n",
    "- Low-stakes decisions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: What You Just Experienced\n",
    "\n",
    "### üéâ Congratulations!\n",
    "\n",
    "You just ran a **real Arbitrium Framework tournament** with actual AI models:\n",
    "\n",
    "1. ‚úÖ **Multiple AI models competed** with real API calls\n",
    "2. ‚úÖ **Competitive pressure** drove higher quality responses\n",
    "3. ‚úÖ **Knowledge Bank preserved** insights from eliminated models\n",
    "4. ‚úÖ **Champion solution synthesized** the best ideas from all perspectives\n",
    "5. ‚úÖ **Full traceability** with downloadable reports\n",
    "6. ‚úÖ **Cost tracking** showed exactly what you spent\n",
    "\n",
    "### üìä Tournament vs. Single Model\n",
    "\n",
    "**When Tournament Wins:**\n",
    "- Multiple valid approaches to explore\n",
    "- High stakes justify the cost\n",
    "- Need defensible audit trail\n",
    "- Synthesis of diverse viewpoints matters\n",
    "\n",
    "**When Single Model is Fine:**\n",
    "- Simple queries\n",
    "- Time-sensitive decisions\n",
    "- Low-stakes problems\n",
    "- Budget constraints\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "**Use Arbitrium Locally:**\n",
    "```bash\n",
    "pip install arbitrium-framework\n",
    "cp config.example.yml config.yml\n",
    "# Edit config.yml with your API keys\n",
    "arbitrium  # Run CLI\n",
    "```\n",
    "\n",
    "**Programmatic Usage:**\n",
    "```python\n",
    "from arbitrium import Arbitrium\n",
    "\n",
    "arbitrium = await Arbitrium.from_config(\"config.yml\")\n",
    "result, metrics = await arbitrium.run_tournament(\"Your question\")\n",
    "```\n",
    "\n",
    "**Learn More:**\n",
    "- üìñ [Documentation](https://github.com/arbitrium-framework/arbitrium)\n",
    "- üéØ [More Examples](https://github.com/arbitrium-framework/arbitrium/tree/main/examples)\n",
    "- üí¨ [Discord Community](https://discord.gg/arbitrium)\n",
    "- üêõ [Report Issues](https://github.com/arbitrium-framework/arbitrium/issues)\n",
    "\n",
    "### üí° Key Takeaways\n",
    "\n",
    "1. **Competitive synthesis works**: Tournament pressure + Knowledge Bank preservation = better outcomes\n",
    "2. **Real cost tracking**: Always know what you're spending\n",
    "3. **Full auditability**: Every decision is traceable\n",
    "4. **Model-agnostic**: Use any combination of OpenAI, Anthropic, Google, etc.\n",
    "5. **Right tool for the job**: Use tournaments for high-stakes decisions, single models for everything else\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for trying Arbitrium Framework!**\n",
    "\n",
    "If this helped solve a real decision for you, consider:\n",
    "- ‚≠ê Starring the [GitHub repo](https://github.com/arbitrium-framework/arbitrium)\n",
    "- üì¢ Sharing your results (with permission)\n",
    "- ü§ù Contributing improvements\n",
    "\n",
    "*Arbitrium Framework‚Ñ¢ - Tournament-Based AI Decision Synthesis*\n",
    "\n",
    "*MIT License | Not affiliated with Arbitrum or Arbitrium RAT*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
